# EEG-Situ_VS_EEG-fMRI

**This project resulted in the presentation of a 45 minute workshop titled 'Unraveling the Mysteries of the Brain: Exploring Visual Event-Related Potentials(ERPs)' at the 7th Portuguese IEEE Bioengineering Conference (ENBENG 2023).**

**Presentation file:** Ap_EEG_fMRI.pdf

**Goal:** Some studies point out EEG degradation when being simultaneously recorded with fMRI. However, does this also happen when the fMRI acquisition is only being simulated? In other words, is placing a subject inside the fMRI machine and simulating the sounds produced during a real acquisition enough to produce visible changes in brain activity during the same cognitive task recorded using solely the EEG? The aim of this study was to investigate and quantify these possible changes since some individuals do experience anxiety inside these machines. If possible without measurable changes or degradation, conjugating the high temporal resolution of the EEG and spatial resolution of the fMRI would provide a more detailed volley of information about the brain's cognitive processes. 

**Acquisition protocol:** 16-channel EEG data was recorded using the Cython + Daisy board from OpenBCI for 2 participants during a face recognition oddball paradigm where in a stream of 120 images containing the faces of 10 different celebrities, subjects had to mentally count each of the 12 instances Daniel Redcliff's face appeared onscreen. The images of Daniel Redcliff's face were categorized as the Target class, while all other celebrities were the Non-Target class. Both were presented with 12 separate blocks of these 120 images, with each face instance lasting 500 ms onscreen followed by a 500 ms blank screen. The EEG responses were evaluated during four different conditions for the same visual paradigm:

1 - Sitting outside the fMRI machine

2 - Sitting outside the fMRI machine while listening to real fMRI sounds 

3 - Lying down inside the fMRI machine  

4 - Lying down inside the fMRI machine with the corresponding sounds

**EEG Analysis:** The Event-Related Potentials (ERPs) N170, Vertical Vertex Potential (VPP), N2 and P3 were then used to assess the cognitive differences in these conditions, both in the Target and Non-Target stimuli. These four conditions were used to isolate all the main variables. The visual processing of these faces (N170 and VPP) and visual attentional mechanisms that are recruited to attend to the Target stimuli (N2 and P3) are being studied. However, when presenting the fMRI sounds we are introducing additional information for the brain to process in the auditory modality. By separating the conditions in this manner, we can more accurately see the effect of these sounds in the brain and establish a more fair comparison between the EEG recordings inside and outside the fMRI machine, discarding the sounds as the only factor affecting the possible differences expected between the no-fMRI and fMRI conditions.

**Preliminary results:** Despite not being statistically quantified yet, the EEG responses clearly show differences in the stimuli processing between the four conditions tested, particularly in the later stages of the visual stream, namely during the P3 period, both in terms of the peak's amplitude and latency, suggesting cognitive differences even when just simulating a fMRI acquisition. However, in earlier potentials, the differences appear negligible.

![EEG_Situ_vs_fMRI](https://github.com/user-attachments/assets/f137bad1-f0bf-4eb5-a098-db1c29368cd8)

